graph TD
    Start([Input Features]) --> Input["Input Layer<br/>feature_dim neurons<br/>Temperature, Demand, etc."]
    
    Input --> Scaler["StandardScaler<br/>Feature Normalization"]
    
    Scaler --> L1["Dense Layer 1<br/>128 neurons<br/>ReLU activation<br/>L2 regularization 0.001"]
    L1 --> D1["Dropout 30%"]
    
    D1 --> L2["Dense Layer 2<br/>64 neurons<br/>ReLU activation<br/>L2 regularization 0.001"]
    L2 --> BN["Batch Normalization"]
    BN --> D2["Dropout 30%"]
    
    D2 --> L3["Dense Layer 3<br/>32 neurons<br/>ReLU activation<br/>L2 regularization 0.001"]
    L3 --> D3["Dropout 20%"]
    
    D3 --> L4["Dense Layer 4<br/>16 neurons<br/>ReLU activation"]
    
    L4 --> Output["Output Layer<br/>1 neuron<br/>Linear activation"]
    
    Output --> Clip["Price Clipping<br/>Training range + 20% margin<br/>Bounds: 10-150 typical<br/>-50 to 250 extended"]
    
    Clip --> Final([Predicted Price])
    
    style Start fill:#3b82f6,stroke:#2563eb,color:#fff
    style Input fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style Scaler fill:#06b6d4,stroke:#0891b2,color:#fff
    style L1 fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style L2 fill:#7c3aed,stroke:#6d28d9,color:#fff
    style L3 fill:#6d28d9,stroke:#5b21b6,color:#fff
    style L4 fill:#5b21b6,stroke:#4c1d95,color:#fff
    style D1 fill:#ef4444,stroke:#dc2626,color:#fff
    style D2 fill:#ef4444,stroke:#dc2626,color:#fff
    style D3 fill:#ef4444,stroke:#dc2626,color:#fff
    style BN fill:#10b981,stroke:#059669,color:#fff
    style Output fill:#22c55e,stroke:#16a34a,color:#fff
    style Clip fill:#eab308,stroke:#ca8a04,color:#000
    style Final fill:#22c55e,stroke:#16a34a,color:#fff